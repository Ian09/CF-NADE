{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import theano \n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from blocks.bricks import Rectifier, Softmax, Identity, NDimensionalSoftmax, Tanh, Logistic, Softplus\n",
    "from blocks.initialization import Constant, Uniform\n",
    "from blocks.bricks import Initializable, Sequence, Feedforward, Linear, Brick\n",
    "from blocks.utils import shared_floatx_nans\n",
    "from blocks.roles import add_role\n",
    "from blocks.roles import WEIGHT, BIAS\n",
    "from blocks.bricks.base import application\n",
    "from toolz import interleave\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "from blocks.bricks.wrappers import WithExtraDims\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.schemes import (SequentialScheme, ShuffledScheme, SequentialExampleScheme, ShuffledExampleScheme)\n",
    "from fuel.transformers import ForceFloatX\n",
    "from fuel.streams import DataStream\n",
    "from hulu_fuel.datasets.MovieLens1M import MovieLens1M\n",
    "from fuel.transformers import Transformer\n",
    "from blocks.extensions import Timing\n",
    "# from mercurial.revset import extra\n",
    "# from blaze.expr.reductions import std\n",
    "\n",
    "def masked_softmax_entropy(h, output_masks, masks):\n",
    "    h -= h.max(axis=1, keepdims=True)\n",
    "    logp = (h - T.log((T.exp(h) * masks).sum(axis=1, keepdims=True))) * masks\n",
    "    return -(output_masks * logp)\n",
    "\n",
    "\n",
    "def convert_onehot_to_gaussian(one_hot_ratings, std=1):\n",
    "    mask = one_hot_ratings.sum(axis=2)\n",
    "    S = np.array([1, 2, 3, 4, 5], dtype='float32')\n",
    "    ratings = T.argmax(one_hot_ratings, axis=2) + 1\n",
    "    scores = ratings.dimshuffle(0, 1, 'x') - S[None, None, :]\n",
    "    unnormalized_score = T.exp(-(scores ** 2) / (2 * std ** 2))\n",
    "    gaussian = mask[:, :, None] * unnormalized_score / (T.sum(unnormalized_score, axis=2)[:, :, None])\n",
    "    return gaussian\n",
    "\n",
    "def rating_cost(pred_score, true_ratings, input_masks, output_masks, D, d, std=1.0, alpha=0.01):\n",
    "    \n",
    "    pred_score_cum = T.extra_ops.cumsum(pred_score, axis=2)\n",
    "    prob_item_ratings = NDimensionalSoftmax(name='rating_cost_sf').apply(pred_score_cum, extra_ndim=1)\n",
    "    accu_prob_1N = T.extra_ops.cumsum(prob_item_ratings, axis=2)\n",
    "    accu_prob_N1 = T.extra_ops.cumsum(prob_item_ratings[:, :, ::-1], axis=2)[:, :, ::-1]\n",
    "    mask1N = T.extra_ops.cumsum(true_ratings[:, :, ::-1], axis=2)[:, :, ::-1]\n",
    "    maskN1 = T.extra_ops.cumsum(true_ratings, axis=2)\n",
    "    cost_ordinal_1N = -T.sum((T.log(prob_item_ratings) - T.log(accu_prob_1N)) * mask1N, axis=2)\n",
    "    cost_ordinal_N1 = -T.sum((T.log(prob_item_ratings) - T.log(accu_prob_N1)) * maskN1, axis=2)\n",
    "    cost_ordinal = cost_ordinal_1N + cost_ordinal_N1\n",
    "    nll_item_ratings = -(true_ratings * T.log(prob_item_ratings)).sum(axis=2)\n",
    "    nll = std * nll_item_ratings.sum(axis=1) * 1.0 * D / (D - d + 1e-6) + alpha * cost_ordinal.sum(axis=1) * 1.0 * D / (D - d + 1e-6)\n",
    "    cost = T.mean(nll)\n",
    "    return cost, nll, nll_item_ratings, cost_ordinal_1N, cost_ordinal_N1, prob_item_ratings\n",
    "\n",
    "def RMSE(pred_ratings, true_ratings):\n",
    "    pass\n",
    "\n",
    "class MovieLensTransformer(Transformer):\n",
    "    \n",
    "    def __init__(self, data_stream, seed=1234):\n",
    "        super(MovieLensTransformer, self).__init__(data_stream)\n",
    "        self.data_sources = ('input_ratings',\n",
    "                             'output_ratings',\n",
    "                             'input_masks',\n",
    "                             'output_masks')\n",
    "        self.produces_examples = False\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_sources\n",
    "    \n",
    "    def get_data(self, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        batch = next(self.child_epoch_iterator)\n",
    "        inp_ratings, out_ratings, input_masks, output_masks = self.preprocess_data(batch)\n",
    "    \n",
    "        return inp_ratings, out_ratings, input_masks, output_masks\n",
    "    \n",
    "    def preprocess_data(self, batch):\n",
    "        \n",
    "        input_ratings, output_ratings, input_masks, output_masks = batch\n",
    "        input_shape = input_ratings.shape\n",
    "        K = 5\n",
    "        input_ratings_3d = np.zeros((input_shape[0], input_shape[1], K), 'int8')\n",
    "        output_ratings_3d = np.zeros_like(input_ratings_3d)\n",
    "        input_ratings_nonzero = input_ratings.nonzero()\n",
    "        input_ratings_3d[input_ratings_nonzero[0],\n",
    "                         input_ratings_nonzero[1],\n",
    "                         input_ratings[input_ratings_nonzero[0],\n",
    "                                       input_ratings_nonzero[1]\n",
    "                                       ] - 1] = 1\n",
    "        output_ratings_nonzero = output_ratings.nonzero()\n",
    "        output_ratings_3d[output_ratings_nonzero[0],\n",
    "                         output_ratings_nonzero[1],\n",
    "                         output_ratings[output_ratings_nonzero[0],\n",
    "                                       output_ratings_nonzero[1]\n",
    "                                       ] - 1] = 1\n",
    "                                       \n",
    "        \n",
    "        \n",
    "        return (input_ratings_3d, output_ratings_3d, input_masks, output_masks)\n",
    "\n",
    "class Trainer_MovieLensTransformer(Transformer):\n",
    "    \n",
    "    def __init__(self, data_stream, seed=1234):\n",
    "        super(Trainer_MovieLensTransformer, self).__init__(data_stream)\n",
    "        self.data_sources = ('input_ratings',\n",
    "                             'output_ratings',\n",
    "                             'input_masks',\n",
    "                             'output_masks')\n",
    "        self.produces_examples = False\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_sources\n",
    "    \n",
    "    def get_data(self, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        batch = next(self.child_epoch_iterator)\n",
    "        inp_ratings, out_ratings, input_masks, output_masks = self.preprocess_data(batch)\n",
    "    \n",
    "        return inp_ratings, out_ratings, input_masks, output_masks\n",
    "    \n",
    "    def preprocess_data(self, batch):\n",
    "        \n",
    "        ratings, _, _, _ = batch\n",
    "#         valid_ratings = np.array(ratings > 0, 'int8')\n",
    "        input_masks = np.zeros_like(ratings)\n",
    "        output_masks = np.zeros_like(ratings)\n",
    "        input_ratings = np.zeros_like(ratings)\n",
    "        output_ratings = np.zeros_like(ratings)\n",
    "        \n",
    "        cnt = 0\n",
    "        for rat in ratings:\n",
    "            print (np.array(ratings).shape)\n",
    "            nonzero_id = rat.nonzero()[0]\n",
    "            if len(nonzero_id) == 0:\n",
    "                continue\n",
    "            ordering = np.random.permutation(np.arange(len(nonzero_id)))\n",
    "            d = np.random.randint(0, len(ordering))\n",
    "            flag_in = (ordering < d)\n",
    "            flag_out = (ordering >= d)\n",
    "            input_masks[cnt][nonzero_id] = flag_in\n",
    "            output_masks[cnt][nonzero_id] = flag_out\n",
    "            input_ratings[cnt] = rat * input_masks[cnt]\n",
    "            output_ratings[cnt] = rat * output_masks[cnt]\n",
    "            cnt += 1\n",
    "        return (input_ratings, output_ratings, input_masks, output_masks)\n",
    "            \n",
    "def get_done_text(start_time):\n",
    "    sys.stdout.flush()\n",
    "    return \"DONE in {:.4f} seconds.\\n\".format(t.time() - start_time)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "class TensorLinear(Initializable):\n",
    "    def __init__(self, input_dim0, input_dim1, output_dim,\n",
    "                 batch_size, **kwargs):\n",
    "        \n",
    "        super(TensorLinear, self).__init__(**kwargs)\n",
    "        self.input_dim0 = input_dim0\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def __allocate(self, input_dim0, input_dim1, output_dim):\n",
    "        W = shared_floatx_nans((input_dim0, input_dim1, output_dim), name='W')\n",
    "        add_role(W, WEIGHT)\n",
    "        self.parameters.append(W)\n",
    "        self.add_auxiliary_variable(W.norm(2), name='W_norm')\n",
    "        b = shared_floatx_nans((output_dim,), name='b')\n",
    "        add_role(b, BIAS)\n",
    "        self.parameters.append(b)\n",
    "        Q = shared_floatx_nans((input_dim0,output_dim),name='Q')\n",
    "        add_role(Q, WEIGHT)\n",
    "        self.parameters.append(Q)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _allocate(self):\n",
    "        self.__allocate(self.input_dim0, self.input_dim1, self.output_dim)\n",
    "        \n",
    "    def _initialize(self):\n",
    "        W, b, Q = self.parameters\n",
    "        self.weights_init.initialize(W, self.rng)\n",
    "        self.biases_init.initialize(b, self.rng)\n",
    "        self.weights_init.initialize(Q, self.rng)\n",
    "\n",
    "    @application(inputs=['input_'], outputs=['output'])\n",
    "    def apply(self, input_): # dim(input) = batch_size * user_num * score\n",
    "        W, b, Q = self.parameters # dim(w) = user_num * score * hidden_num\n",
    "#         input_ = input_ / (T.sum(input_, axis=(1,2))[:, None, None]+1e-6)\n",
    "        output_ = T.tensordot(input_, W, axes=[[1, 2], [0, 1]]) + b\n",
    "        input_mask = T.sum(input_, axis = 2)\n",
    "        output_masked = T.dot(input_mask, Q) #dim(output) = batch_size * hidden_num\n",
    "        output = output_ + output_masked\n",
    "#         output = output / (T.sum(input_, axis=(1,2))[:,None] + 1)\n",
    "        return output\n",
    "    \n",
    "    def get_dim(self, name):\n",
    "        if name == 'input_':\n",
    "            return (self.input_dim0, self.input_dim1)\n",
    "        if name == 'output':\n",
    "            return self.output_dim\n",
    "        super(TensorLinear, self).get_dim(name)\n",
    "\n",
    "class TensorLinear_inverse(Initializable):\n",
    "    def __init__(self, input_dim, output_dim0, output_dim1,\n",
    "                 batch_size, **kwargs):\n",
    "        \n",
    "        super(TensorLinear_inverse, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim0 = output_dim0\n",
    "        self.output_dim1 = output_dim1\n",
    "        \n",
    "    def __allocate(self, input_dim, output_dim0, output_dim1):\n",
    "        W = shared_floatx_nans((input_dim, output_dim0, output_dim1), name='W') # hidden_num * user_num * score\n",
    "        add_role(W, WEIGHT)\n",
    "        self.parameters.append(W)\n",
    "        self.add_auxiliary_variable(W.norm(2), name='W_norm')\n",
    "        b = shared_floatx_nans((output_dim0, output_dim1), name='b')\n",
    "        add_role(b, BIAS)\n",
    "        self.parameters.append(b)\n",
    "        \n",
    "    \n",
    "    def _allocate(self):\n",
    "        self.__allocate(self.input_dim, self.output_dim0, self.output_dim1)\n",
    "        \n",
    "    def _initialize(self):\n",
    "        W, b = self.parameters\n",
    "        self.weights_init.initialize(W, self.rng)\n",
    "        self.biases_init.initialize(b, self.rng)\n",
    "\n",
    "    @application(inputs=['input_'], outputs=['output'])\n",
    "    def apply(self, input_):\n",
    "        W, b = self.parameters\n",
    "        output = T.tensordot(input_, W, axes=[[1], [0]]) + b #batch_size * hidden_num\n",
    "        return output\n",
    "    \n",
    "    def get_dim(self, name):\n",
    "        if name == 'input_':\n",
    "            return self.input_dim\n",
    "        if name == 'output':\n",
    "            return (self.output_dim0, self.output_dim1)\n",
    "        super(TensorLinear_inverse, self).get_dim(name)\n",
    "        \n",
    "class TensorLinear_Plus_Linear(Initializable):\n",
    "    def __init__(self, input_dim0, input_dim1, output_dim,\n",
    "                 batch_size, **kwargs):\n",
    "        '''\n",
    "        input_dim0: number of items\n",
    "        input_dim1: number of ratings (1~input_dim1), a.k.a K in our paper\n",
    "        '''\n",
    "        super(TensorLinear_Plus_Linear, self).__init__(**kwargs)\n",
    "        self.input_dim0 = input_dim0\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def __allocate(self, input_dim0, input_dim1, output_dim):\n",
    "        W = shared_floatx_nans((input_dim0, input_dim1, output_dim), name='W')\n",
    "        add_role(W, WEIGHT)\n",
    "        self.parameters.append(W)\n",
    "        self.add_auxiliary_variable(W.norm(2), name='W_norm')\n",
    "        b = shared_floatx_nans((output_dim,), name='b')\n",
    "        add_role(b, BIAS)\n",
    "        self.parameters.append(b)\n",
    "        W_Linear = shared_floatx_nans((input_dim0, output_dim), name='W_Linear')\n",
    "        self.add_auxiliary_variable(W_Linear.norm(2), name='W_Linear_norm')\n",
    "        add_role(W_Linear, WEIGHT)\n",
    "        self.parameters.append(W_Linear)\n",
    "        \n",
    "    \n",
    "    def _allocate(self):\n",
    "        self.__allocate(self.input_dim0, self.input_dim1, self.output_dim)\n",
    "        \n",
    "    def _initialize(self):\n",
    "        W, b, W_Linear = self.parameters\n",
    "        self.weights_init.initialize(W, self.rng)\n",
    "        self.biases_init.initialize(b, self.rng)\n",
    "        self.weights_init.initialize(W_Linear, self.rng)\n",
    "\n",
    "    @application(inputs=['input0_', 'input1_'], outputs=['output'])\n",
    "    def apply(self, input0_, input1_):\n",
    "        W, b, W_Linear = self.parameters\n",
    "        output = T.tensordot(input0_, W, axes=[[1, 2], [0, 1]]) + T.dot(input1_, W_Linear) + b\n",
    "        return output\n",
    "    \n",
    "    def get_dim(self, name):\n",
    "        if name == 'input_':\n",
    "            return (self.input_dim0, self.input_dim1)\n",
    "        if name == 'output':\n",
    "            return self.output_dim\n",
    "        super(TensorLinear_Plus_Linear, self).get_dim(name)\n",
    "\n",
    "\n",
    "def Adam_optimizer(input_list, cost, parameters, lr0, b1, b2, epsilon):\n",
    "    \n",
    "    params_gradient = [T.grad(cost, param) for param in parameters]\n",
    "    grad_shared = [theano.shared(p.get_value() * 0., name='%s_grad' % p.name) for p in parameters]\n",
    "    grads_update = [(gs, g) for gs, g in zip(grad_shared, params_gradient)]\n",
    "    f_get_grad = theano.function(inputs=input_list,\n",
    "                                  updates=grads_update,\n",
    "                                  outputs=cost,\n",
    "                                  )\n",
    "    \n",
    "    updates = []\n",
    "    \n",
    "    i = theano.shared(np.float32(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1 ** (i_t)\n",
    "    fix2 = 1. - b2 ** (i_t)\n",
    "    lr_t = lr0 * (T.sqrt(fix2) / fix1)\n",
    "\n",
    "    for p, g in zip(parameters, grad_shared):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + epsilon)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    updates.append((i, i_t))\n",
    "\n",
    "    f_update_parameters = theano.function([lr0], [], updates=updates)\n",
    "    \n",
    "    return f_get_grad, f_update_parameters, grad_shared\n",
    "\n",
    "def Adadelta_optimizer(input_list, cost, parameters, decay, epsilon):\n",
    "    \n",
    "    params_gradient = [T.grad(cost, param) for param in parameters]\n",
    "    grad_shared = [theano.shared(p.get_value() * 0., name='%s_grad' % p.name) for p in parameters]\n",
    "    running_up2 = [theano.shared(p.get_value() * 0, name='%s_rup2' % p.name) for p in parameters]\n",
    "    running_grads2 = [theano.shared(p.get_value() * 0, name='%s_rgrad2' % p.name) for p in parameters]\n",
    "    zgup = [(zg, g) for zg, g in zip(grad_shared, params_gradient)]\n",
    "    rg2up = [(rg2, decay * rg2 + (1.0 - decay) * (g ** 2)) for rg2, g in zip(running_grads2, params_gradient)]\n",
    "    \n",
    "    f_get_grad = theano.function(inputs=input_list,\n",
    "                                  updates=zgup + rg2up,\n",
    "                                  outputs=cost,\n",
    "                                  )\n",
    "    \n",
    "    updir = [-T.sqrt(ru2 + epsilon) / T.sqrt(rg2 + epsilon) * zg for zg, ru2, rg2 in zip(grad_shared, running_up2, running_grads2)]\n",
    "    ru2up = [(ru2, decay * ru2 + (1 - decay) * (ud ** 2)) for ru2, ud in zip(running_up2, updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(parameters, updir)]\n",
    "    \n",
    "    \n",
    "\n",
    "    f_update_parameters = theano.function([], [], updates=ru2up + param_up)\n",
    "    \n",
    "    return f_get_grad, f_update_parameters, grad_shared\n",
    "    \n",
    "def SGD_optimizer(input_list, cost, parameters, lr0, mu):\n",
    "    \n",
    "    params_gradient = [T.grad(cost, param) for param in parameters]\n",
    "    grad_shared = [theano.shared(p.get_value() * 0., name='%s_grad' % p.name) for p in parameters]\n",
    "    velo_shared = [theano.shared(p.get_value() * 0., name='%s_velocity' % p.name) for p in parameters]\n",
    "    \n",
    "    grads_update = [(gs, g) for gs, g in zip(grad_shared, params_gradient)]\n",
    "    f_get_grad = theano.function(inputs=input_list,\n",
    "                                  updates=grads_update,\n",
    "                                  outputs=cost,\n",
    "                                  )\n",
    "    \n",
    "    updates = []\n",
    "    for p, v, g in zip(parameters, velo_shared, grad_shared):\n",
    "        \n",
    "        p_t = p - lr0 * g + mu * v\n",
    "        v_t = mu * v - lr0 * g\n",
    "        updates.append((p, p_t))\n",
    "        updates.append((v, v_t))\n",
    "\n",
    "    f_update_parameters = theano.function([lr0], [], updates=updates)\n",
    "    \n",
    "    return f_get_grad, f_update_parameters, grad_shared  \n",
    "\n",
    "def polyak(parameters, mu):\n",
    "    \n",
    "    polyak_shared = [theano.shared(p.get_value(), name='%s_polyak' % p.name) for p in parameters]\n",
    "    updates = []\n",
    "    for y, p in zip(polyak_shared, parameters):\n",
    "        y_t = mu * y + (1 - mu) * p\n",
    "        updates.append((y, y_t))\n",
    "    f_update_polyak = theano.function([], [], updates=updates)\n",
    "    \n",
    "    return f_update_polyak, polyak_shared\n",
    "\n",
    "def polyak_replace(parameters, polyaks):\n",
    "    \n",
    "    updates = []\n",
    "    for y, p in zip(polyaks, parameters):\n",
    "        y_name_split = y.name.split('_')\n",
    "        \n",
    "        assert y_name_split[0] == p.name\n",
    "        updates.append((p, y))\n",
    "    \n",
    "    f_replace_polyak = theano.function([], [], updates=updates)\n",
    "    return f_replace_polyak\n",
    "        \n",
    "\n",
    "class tabula_NADE(Sequence, Initializable, Feedforward):\n",
    "    \n",
    "    def __init__(self, input_dim0, input_dim1, other_dims, activations, batch_size,\n",
    "                 **kwargs):\n",
    "        \n",
    "        self.activations = activations\n",
    "        self.input_dim0 = input_dim0\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.other_dims = other_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.linear_transformations = []\n",
    "        self.linear_transformations.append(TensorLinear(input_dim0=self.input_dim0,\n",
    "                                                        input_dim1=self.input_dim1,\n",
    "                                                        output_dim=self.other_dims[0], #500\n",
    "                                                        batch_size=batch_size)\n",
    "                                           )\n",
    "        self.linear_transformations.extend([Linear(name='linear_{}'.format(i),\n",
    "                                                   input_dim=other_dims[i],\n",
    "                                                   output_dim=other_dims[i + 1])\n",
    "                                            for i in range(len(other_dims) - 1)])\n",
    "        self.linear_transformations.append(TensorLinear_inverse(input_dim=self.other_dims[-1],\n",
    "                                                                output_dim0=self.input_dim0,\n",
    "                                                                output_dim1=self.input_dim1,\n",
    "                                                                batch_size=batch_size))\n",
    "        application_methods = []\n",
    "        for entity in interleave([self.linear_transformations, activations]):\n",
    "            if entity is None:\n",
    "                continue\n",
    "            if isinstance(entity, Brick):\n",
    "                application_methods.append(entity.apply)\n",
    "            else:\n",
    "                application_methods.append(entity)\n",
    "        super(tabula_NADE, self).__init__(application_methods, **kwargs)\n",
    "        \n",
    "\n",
    "        \n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return (self.input_dim0, input_dim1)\n",
    "\n",
    "    @input_dim.setter\n",
    "    def input_dim(self, value):\n",
    "        self.input_dim0 = value[0]\n",
    "        self.input_dim1 = value[1]\n",
    "\n",
    "    @property\n",
    "    def hidden_dims(self):\n",
    "        return self.other_dims\n",
    "\n",
    "    @hidden_dims.setter\n",
    "    def hidden_dims(self, value):\n",
    "        self.other_dims = value\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    import sys, os\n",
    "    import time as t\n",
    "    import copy as cp\n",
    "    import gc\n",
    "    from blocks.graph import ComputationGraph, apply_dropout\n",
    "    from blocks.filter import VariableFilter\n",
    "    from blocks.roles import INPUT, OUTPUT\n",
    "    sys.argv.pop(0)\n",
    "    \n",
    "    \n",
    "    input_dim0 = 6040\n",
    "    input_dim1 = 5\n",
    "#     output_dim = 128\n",
    "    batch_size = int(sys.argv[0])\n",
    "    n_iter = int(sys.argv[1])\n",
    "    look_ahead = int(sys.argv[2])\n",
    "    lr = float(sys.argv[3])  # lr in Adam and SGD, decay in Adadelta\n",
    "    b1 = float(sys.argv[4])  # b1 in Adam, mu in SGD\n",
    "    b2 = float(sys.argv[5])\n",
    "    epsilon = float(sys.argv[6])\n",
    "    hidden_size_split = (sys.argv[7]).split('_')\n",
    "    hidden_size = [int(x) for x in hidden_size_split]\n",
    "    activation_function = sys.argv[8]\n",
    "    drop_rate = float(sys.argv[9])\n",
    "    weight_decay = float(sys.argv[10])\n",
    "    Optimizer = sys.argv[11]\n",
    "    std = float(sys.argv[12])\n",
    "    alpha = float(sys.argv[13])\n",
    "    polyak_mu = float(sys.argv[14])\n",
    "    output_path = sys.argv[15]\n",
    "    \n",
    "    \n",
    "    np.random.seed(12345)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     input_mat = np.random.rand(batch_size, input_dim0, input_dim1).astype(theano.config.floatX)\n",
    "#     input_ratings = np.zeros((batch_size, input_dim0, input_dim1), dtype=theano.config.floatX)\n",
    "#     input_ratings[:, :, 0] = 1\n",
    "    trainset = MovieLens1M(which_set=['train'], sources=('input_ratings', 'output_ratings', 'input_masks', 'output_masks'))\n",
    "    validset = MovieLens1M(which_set=['valid'], sources=('input_ratings', 'output_ratings', 'input_masks', 'output_masks'))\n",
    "    testset = MovieLens1M(which_set=['test'], sources=('input_ratings', 'output_ratings', 'input_masks', 'output_masks'))\n",
    "    \n",
    "    train_loop_stream = ForceFloatX(\n",
    "                                    data_stream=MovieLensTransformer(\n",
    "                                                                     \n",
    "                                            data_stream=Trainer_MovieLensTransformer(\n",
    "                                                                                     data_stream=DataStream(\n",
    "                                                                                                            dataset=trainset,\n",
    "                                                                                                            iteration_scheme=ShuffledScheme(\n",
    "                                                                                                                                            trainset.num_examples,\n",
    "                                                                                                                                            batch_size\n",
    "                                                                                                                                            )\n",
    "                                                                                                            )\n",
    "                                                                                     )\n",
    "                                                                     )\n",
    "                                    )\n",
    "    \n",
    "    trainval_loop_stream = ForceFloatX(\n",
    "                                    data_stream=MovieLensTransformer(\n",
    "                                                                     \n",
    "                                            data_stream=Trainer_MovieLensTransformer(\n",
    "                                                                                     data_stream=DataStream(\n",
    "                                                                                                            dataset=testset,\n",
    "                                                                                                            iteration_scheme=ShuffledScheme(\n",
    "                                                                                                                                            trainset.num_examples,\n",
    "                                                                                                                                            batch_size\n",
    "                                                                                                                                            )\n",
    "                                                                                                            )\n",
    "                                                                                     )\n",
    "                                                                     )\n",
    "                                    )\n",
    "    \n",
    "#     train_monitor_stream = ForceFloatX(\n",
    "#                             data_stream=MovieLensTransformer(\n",
    "#                                                     data_stream=DataStream(\n",
    "#                                                                            dataset=trainset,\n",
    "#                                                                            iteration_scheme=ShuffledScheme(\n",
    "#                                                                                                            validset.num_examples,\n",
    "#                                                                                                            batch_size\n",
    "#                                                                                                            )\n",
    "#                                                                            \n",
    "#                                                                            )\n",
    "#                                                     \n",
    "#                                                     )\n",
    "#                                        )\n",
    "    \n",
    "    valid_monitor_stream = ForceFloatX(\n",
    "                            data_stream=MovieLensTransformer(\n",
    "                                                    data_stream=DataStream(\n",
    "                                                                           dataset=validset,\n",
    "                                                                           iteration_scheme=ShuffledScheme(\n",
    "                                                                                                           validset.num_examples,\n",
    "                                                                                                           batch_size\n",
    "                                                                                                           )\n",
    "                                                                           \n",
    "                                                                           )\n",
    "                                                    \n",
    "                                                    )\n",
    "                                       )\n",
    "    \n",
    "    test_monitor_stream = ForceFloatX(\n",
    "                            data_stream=MovieLensTransformer(\n",
    "                                                    data_stream=DataStream(\n",
    "                                                                           dataset=testset,\n",
    "                                                                           iteration_scheme=ShuffledScheme(\n",
    "                                                                                                           testset.num_examples,\n",
    "                                                                                                           batch_size\n",
    "                                                                                                           )\n",
    "                                                                           \n",
    "                                                                           )\n",
    "                                                    \n",
    "                                                    )\n",
    "                                       )\n",
    "    \n",
    "    rating_freq = np.zeros((6040, 5)) #users' rating records\n",
    "    init_b = np.zeros((6040, 5))\n",
    "    for batch in valid_monitor_stream.get_epoch_iterator():\n",
    "        inp_r, out_r, inp_m, out_m = batch # dim(inp_r) = (batch_size , 6040, 5), user_num = 6040, movie_num = 3706\n",
    "        rating_freq += inp_r.sum(axis=0)\n",
    "    #print (rating_freq[:10])\n",
    "    \n",
    "    log_rating_freq = np.log(rating_freq + 1e-8)\n",
    "    log_rating_freq_diff = np.diff(log_rating_freq, axis=1)\n",
    "    init_b[:, 1:] = log_rating_freq_diff\n",
    "    init_b[:, 0] = log_rating_freq[:, 0]\n",
    "#     init_b = np.log(rating_freq / (rating_freq.sum(axis=1)[:, None] + 1e-8) +1e-8)  * (rating_freq>0)\n",
    "    \n",
    "    new_items = np.where(rating_freq.sum(axis=1) == 0)[0]\n",
    "    \n",
    "    input_ratings = T.tensor3(name='input_ratings', dtype=theano.config.floatX)\n",
    "    output_ratings = T.tensor3(name='output_ratings', dtype=theano.config.floatX)\n",
    "    input_masks = T.matrix(name='input_masks', dtype=theano.config.floatX)\n",
    "    output_masks = T.matrix(name='output_masks', dtype=theano.config.floatX)\n",
    "    \n",
    "    input_ratings_cum = T.extra_ops.cumsum(input_ratings[:, :, ::-1], axis=2)[:, :, ::-1]\n",
    "    \n",
    "    \n",
    "#     hidden_size = [256]\n",
    "    if activation_function == 'reclin':\n",
    "        act = Rectifier\n",
    "    elif activation_function == 'tanh':\n",
    "        act = Tanh\n",
    "    elif activation_function == 'sigmoid':\n",
    "        act = Logistic\n",
    "    elif activation_function == 'softplus':\n",
    "        act = Softplus\n",
    "    layers_act = [act('layer_%d' % i) for i in range(len(hidden_size))]\n",
    "    NADE_CF_model = tabula_NADE(activations=layers_act,\n",
    "                                input_dim0=input_dim0,\n",
    "                                input_dim1=input_dim1,\n",
    "                                other_dims=hidden_size, \n",
    "                                batch_size=batch_size,\n",
    "                                weights_init=Uniform(std=0.05),\n",
    "                                biases_init=Constant(0.0)\n",
    "                                )\n",
    "    NADE_CF_model.push_initialization_config()\n",
    "    dims = [input_dim0] + hidden_size + [input_dim0]\n",
    "    linear_layers = [layer for layer in NADE_CF_model.children\n",
    "                     if 'linear' in layer.name]\n",
    "    assert len(linear_layers) == len(dims) - 1\n",
    "    for i in range(len(linear_layers)):\n",
    "        H1 = dims[i]\n",
    "        H2 = dims[i + 1]\n",
    "        width = 2 * np.sqrt(6) / np.sqrt(H1 + H2)\n",
    "#         std = np.sqrt(2. / dim)\n",
    "        linear_layers[i].weights_init = Uniform(width=width)\n",
    "    \n",
    "    \n",
    "#     NADE_CF_model.children[0].weights_init = Constant(1)\n",
    "#     NADE_CF_model.children[0].biases_init = Constant(1.5)\n",
    "#     NADE_CF_model.children[1].weights_init = Constant(2)\n",
    "#     NADE_CF_model.children[1].biases_init = Constant(2.5)\n",
    "    NADE_CF_model.initialize()\n",
    "    NADE_CF_model.children[-1].parameters[-1].set_value(init_b.astype(theano.config.floatX))\n",
    "    y = NADE_CF_model.apply(input_ratings_cum)\n",
    "    y_cum = T.extra_ops.cumsum(y, axis=2)\n",
    "    predicted_ratings = NDimensionalSoftmax().apply(y_cum, extra_ndim=1)\n",
    "    d = input_masks.sum(axis=1)\n",
    "    D = (input_masks + output_masks).sum(axis=1)\n",
    "#     ratings = T.tensor3(name=\"ratings\", dtype=theano.config.floatX)\n",
    "    cost, nll, nll_item_ratings, cost_ordinal_1N, cost_ordinal_N1, prob_item_ratings = rating_cost(y, output_ratings, input_masks, output_masks, D, d, alpha=alpha, std=std)\n",
    "    cost.name = 'cost'\n",
    "    \n",
    "    cg = ComputationGraph(cost)\n",
    "    if weight_decay > 0.0:\n",
    "        all_weights = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "        l2_weights = T.sum([(W ** 2).sum() for W in all_weights])\n",
    "        l2_cost = cost + weight_decay * l2_weights\n",
    "        l2_cost.name = 'l2_decay_' + cost.name\n",
    "        cg = ComputationGraph(l2_cost)\n",
    "    if drop_rate > 0.0:\n",
    "        dropped_layer = VariableFilter(roles=[INPUT], bricks=NADE_CF_model.children)(cg.variables)\n",
    "        dropped_layer = [layer for layer in dropped_layer if 'linear' in layer.name]\n",
    "        dropped_layer = dropped_layer[1:]\n",
    "        cg_dropout = apply_dropout(cg, dropped_layer, drop_rate)\n",
    "    else:\n",
    "        cg_dropout = cg\n",
    "    training_cost = cg_dropout.outputs[0]\n",
    "    lr0 = T.scalar(name='learning_rate', dtype=theano.config.floatX)\n",
    "    input_list = [input_ratings, input_masks, output_ratings, output_masks]\n",
    "    if Optimizer == 'Adam':\n",
    "        f_get_grad, f_update_parameters, shared_gradients = Adam_optimizer(input_list,\n",
    "                                                                           training_cost,\n",
    "                                                                           cg_dropout.parameters,\n",
    "                                                                           lr0,\n",
    "                                                                           b1,\n",
    "                                                                           b2,\n",
    "                                                                           epsilon)\n",
    "    elif Optimizer == 'Adadelta':\n",
    "        f_get_grad, f_update_parameters, shared_gradients = Adadelta_optimizer(input_list,\n",
    "                                                                               training_cost,\n",
    "                                                                               cg_dropout.parameters,\n",
    "                                                                               lr,\n",
    "                                                                               epsilon)\n",
    "    elif Optimizer == 'SGD':\n",
    "        f_get_grad, f_update_parameters, shared_gradients = SGD_optimizer(input_list,\n",
    "                                                                           training_cost,\n",
    "                                                                           cg_dropout.parameters,\n",
    "                                                                           lr0,\n",
    "                                                                           b1)\n",
    "        \n",
    "        \n",
    "#     f_get_grad, f_update_parameters, shared_gradients = SGD_optimizer(input_list,\n",
    "#                                                                       cost,\n",
    "#                                                                       cg.parameters,\n",
    "#                                                                       lr0)\n",
    "    param_list = []\n",
    "    [param_list.extend(p.parameters) for p in NADE_CF_model.children]\n",
    "    f_update_polyak, shared_polyak = polyak(param_list, mu=polyak_mu)\n",
    "    \n",
    "#     f_monitor = theano.function(inputs=[input_ratings, input_masks, output_ratings, output_masks],\n",
    "#                                 outputs=[predicted_ratings,training_cost, nll, nll_item_ratings, cost_ordinal_1N, cost_ordinal_N1, prob_item_ratings])\n",
    "    f_monitor = theano.function(inputs=[input_ratings],\n",
    "                                outputs=[predicted_ratings])\n",
    "\n",
    "    nb_of_epocs_without_improvement = 0\n",
    "    best_valid_error = np.Inf\n",
    "    epoch = 0\n",
    "    test_RMSE = []\n",
    "    valid_RMSE = []\n",
    "    test_RMSE = []\n",
    "    best_model = cp.deepcopy(NADE_CF_model)\n",
    "    best_polyak = cp.deepcopy(shared_polyak)\n",
    "    start_training_time = t.time()\n",
    "    lr_tracer = []\n",
    "    rate_score = np.array([1, 2, 3, 4, 5], np.float32)\n",
    "    while(epoch < n_iter and nb_of_epocs_without_improvement < look_ahead):        \n",
    "        print('Epoch {0}'.format(epoch))\n",
    "        epoch += 1\n",
    "        start_time_epoch = t.time()\n",
    "        cost_train = []\n",
    "        squared_error_train = []\n",
    "        n_sample_train = []\n",
    "        cntt = 0\n",
    "        train_time = 0\n",
    "        for batch in train_loop_stream.get_epoch_iterator():\n",
    "            \n",
    "            inp_r, out_r, inp_m, out_m = batch\n",
    "            #print(inp_r.shape, out_r.shape, inp_m.shape, out_m.shape)\n",
    "            train_t = t.time()\n",
    "            cost_value = f_get_grad(inp_r, inp_m, out_r, out_m)\n",
    "            train_time += t.time() - train_t\n",
    "#             pred_ratings = f_monitor(inp_r)\n",
    "            if Optimizer == 'Adadelta':\n",
    "                f_update_parameters()\n",
    "            else:\n",
    "                f_update_parameters(lr)\n",
    "            f_update_polyak()\n",
    "            pred_ratings = f_monitor(inp_r)\n",
    "            #print (np.array(pred_ratings).shape)\n",
    "            true_r = out_r.argmax(axis=2) + 1\n",
    "            pred_r = (pred_ratings[0] * rate_score[np.newaxis, np.newaxis, :]).sum(axis=2)\n",
    "            pred_r[:, new_items] = 3\n",
    "#             new_users = np.where((inp_m+out_m).sum(axis=1)==0)[0]\n",
    "#             pred_r[new_users,:] = 3\n",
    "#             pred_r = pred_ratings[0].argmax(axis=2) + 1\n",
    "            mask = out_r.sum(axis=2)\n",
    "            se = np.sum(np.square(true_r - pred_r) * mask)\n",
    "            n = np.sum(mask)\n",
    "            squared_error_train.append(se)\n",
    "            n_sample_train.append(n)\n",
    "            cost_train.append(cost_value)\n",
    "            cntt+= 1\n",
    "            \n",
    "        cost_train = np.array(cost_train).mean()\n",
    "        squared_error_ = np.array(squared_error_train).sum()\n",
    "        n_samples = np.array(n_sample_train).sum()\n",
    "        train_RMSE = np.sqrt(squared_error_ / (n_samples * 1.0 + 1e-8))\n",
    "        \n",
    "        print('\\tTraining   ...', end=' ')\n",
    "        print('Train     :', \"RMSE: {0:.6f}\".format(train_RMSE), \" Cost Error: {0:.6f}\".format(cost_train), \"Train Time: {0:.6f}\".format(train_time), get_done_text(start_time_epoch))\n",
    "        \n",
    "        print('\\tValidating ...', end=' ')\n",
    "        start_time = t.time()\n",
    "        squared_error_valid = []\n",
    "        n_sample_valid = []\n",
    "        valid_time = 0\n",
    "        for batch in valid_monitor_stream.get_epoch_iterator():\n",
    "            inp_r, out_r, inp_m, out_m = batch\n",
    "            valid_t = t.time()\n",
    "            pred_ratings = f_monitor(inp_r)\n",
    "            valid_time += t.time() - valid_t\n",
    "            true_r = out_r.argmax(axis=2) + 1\n",
    "            pred_r = (pred_ratings[0] * rate_score[np.newaxis, np.newaxis, :]).sum(axis=2)\n",
    "            \n",
    "            pred_r[:, new_items] = 3\n",
    "#             new_users = np.where(inp_m.sum(axis=1)==0)[0]\n",
    "#             pred_r[new_users,:] = 3\n",
    "            \n",
    "#             pred_r = pred_ratings[0].argmax(axis=2) + 1\n",
    "            mask = out_r.sum(axis=2)\n",
    "            se = np.sum(np.square(true_r - pred_r) * mask)\n",
    "            n = np.sum(mask)\n",
    "            squared_error_valid.append(se)\n",
    "            n_sample_valid.append(n)\n",
    "        \n",
    "        squared_error_ = np.array(squared_error_valid).sum()\n",
    "        n_samples = np.array(n_sample_valid).sum()\n",
    "        valid_RMSE = np.sqrt(squared_error_ / (n_samples * 1.0 + 1e-8))\n",
    "        print('Validation:', \" RMSE: {0:.6f}\".format(valid_RMSE) , \"Valid Time: {0:.6f}\".format(valid_time), get_done_text(start_time), end=' ')\n",
    "        if valid_RMSE < best_valid_error:\n",
    "            best_epoch = epoch\n",
    "            nb_of_epocs_without_improvement = 0\n",
    "            best_valid_error = valid_RMSE\n",
    "            del best_model\n",
    "            del best_polyak\n",
    "            gc.collect()\n",
    "            \n",
    "            best_model = cp.deepcopy(NADE_CF_model)\n",
    "            best_polyak = cp.deepcopy(shared_polyak)\n",
    "            print('\\n\\n Got a good one')\n",
    "        else:\n",
    "            nb_of_epocs_without_improvement += 1\n",
    "            if Optimizer == 'Adadelta':\n",
    "                pass\n",
    "            elif nb_of_epocs_without_improvement == look_ahead and lr > 1e-5:\n",
    "                nb_of_epocs_without_improvement = 0\n",
    "                lr /= 4 \n",
    "                print(\"learning rate is now %s\" % lr) \n",
    "        lr_tracer.append(lr)\n",
    "                \n",
    "                \n",
    "    print('\\n### Training, n_layers=%d' % (len(hidden_size)), get_done_text(start_training_time))\n",
    "    \n",
    "    best_y = best_model.apply(input_ratings_cum)\n",
    "    best_y_cum = T.extra_ops.cumsum(best_y, axis=2)\n",
    "    best_predicted_ratings = NDimensionalSoftmax().apply(best_y_cum, extra_ndim=1)\n",
    "    f_monitor_best = theano.function(inputs=[input_ratings],\n",
    "                                outputs=[best_predicted_ratings])\n",
    "    \n",
    "    print('\\tTesting ...', end=' ')\n",
    "    start_time = t.time()\n",
    "    squared_error_test = []\n",
    "    n_sample_test = []\n",
    "    test_time = 0\n",
    "    for batch in test_monitor_stream.get_epoch_iterator():\n",
    "        inp_r, out_r, inp_m, _ = batch\n",
    "        test_t = t.time()\n",
    "        pred_ratings = f_monitor_best(inp_r)\n",
    "        test_time += t.time() - test_t\n",
    "        true_r = out_r.argmax(axis=2) + 1\n",
    "        pred_r = (pred_ratings[0] * rate_score[np.newaxis, np.newaxis, :]).sum(axis=2)\n",
    "        pred_r[:, new_items] = 3\n",
    "#         new_users = np.where(inp_m.sum(axis=1)==0)[0]\n",
    "#         pred_r[new_users,:] = 3\n",
    "#         pred_r = pred_ratings[0].argmax(axis=2) + 1\n",
    "        mask = out_r.sum(axis=2)\n",
    "        se = np.sum(np.square(true_r - pred_r) * mask)\n",
    "        n = np.sum(mask)\n",
    "        squared_error_test.append(se)\n",
    "        n_sample_test.append(n)\n",
    "    \n",
    "    squared_error_ = np.array(squared_error_test).sum()\n",
    "    n_samples = np.array(n_sample_test).sum()\n",
    "    test_RMSE = np.sqrt(squared_error_ / (n_samples * 1.0 + 1e-8))\n",
    "    print('Test:', \" RMSE: {0:.6f}\".format(test_RMSE) , \"Test Time: {0:.6f}\".format(test_time), get_done_text(start_time), end=' ')\n",
    "    \n",
    "    f = open(os.path.join(output_path, 'Reco_NADE_masked_directly_itembased.txt'), 'a')\n",
    "    to_write = [str(test_RMSE), str(best_valid_error), str(best_epoch)] + sys.argv[:-1]\n",
    "    line = \" \".join(to_write) + '\\n'\n",
    "    f.write(line)\n",
    "    f.close()\n",
    "    \n",
    "    print('\\tTesting with polyak parameters...', end=' ')\n",
    "    best_param_list = []\n",
    "    [best_param_list.extend(p.parameters) for p in best_model.children]\n",
    "    f_replace = polyak_replace(best_param_list, best_polyak)\n",
    "    f_replace()\n",
    "    cc = 0\n",
    "    for pp in best_polyak:\n",
    "        pp_value = pp.get_value()\n",
    "        np.save('./tmp/cfnade/%d'%cc, pp_value)\n",
    "        cc+=1\n",
    "    \n",
    "    \n",
    "    start_time = t.time()\n",
    "    squared_error_test = []\n",
    "    n_sample_test = []\n",
    "    test_time = 0\n",
    "    for batch in test_monitor_stream.get_epoch_iterator():\n",
    "        inp_r, out_r, inp_m, _ = batch\n",
    "        test_t = t.time()\n",
    "        pred_ratings = f_monitor_best(inp_r)\n",
    "        test_time += t.time() - test_t\n",
    "        true_r = out_r.argmax(axis=2) + 1\n",
    "        pred_r = (pred_ratings[0] * rate_score[np.newaxis, np.newaxis, :]).sum(axis=2)\n",
    "        pred_r[:, new_items] = 3\n",
    "#         new_users = np.where(inp_m.sum(axis=1)==0)[0]\n",
    "#         pred_r[new_users,:] = 3\n",
    "#         pred_r = pred_ratings[0].argmax(axis=2) + 1\n",
    "        mask = out_r.sum(axis=2)\n",
    "        se = np.sum(np.square(true_r - pred_r) * mask)\n",
    "        n = np.sum(mask)\n",
    "        squared_error_test.append(se)\n",
    "        n_sample_test.append(n)\n",
    "    \n",
    "    squared_error_ = np.array(squared_error_test).sum()\n",
    "    n_samples = np.array(n_sample_test).sum()\n",
    "    test_RMSE = np.sqrt(squared_error_ / (n_samples * 1.0 + 1e-8))\n",
    "    print('Test:', \" RMSE: {0:.6f}\".format(test_RMSE) , \"Test Time: {0:.6f}\".format(test_time), get_done_text(start_time), end=' ')\n",
    "    \n",
    "    f = open(os.path.join(output_path, 'Reco_NADE_masked_directly_itembased.txt'), 'a')\n",
    "    to_write = [str(test_RMSE), str(best_valid_error), str(best_epoch)] + sys.argv[:-1] + ['polyak']\n",
    "    line = \" \".join(to_write) + '\\n'\n",
    "    f.write(line)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
